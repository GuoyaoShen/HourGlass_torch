{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main_train_torch2.0.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"S7RK6OckO4q4","colab_type":"code","outputId":"69f93b8a-b911-4b87-c732-5bd5eeb5f87a","executionInfo":{"status":"ok","timestamp":1563936910075,"user_tz":240,"elapsed":54709,"user":{"displayName":"Guoyao Shen","photoUrl":"https://lh5.googleusercontent.com/-n2iQosNzLWU/AAAAAAAAAAI/AAAAAAAAAAk/8oJH0jttUB8/s64/photo.jpg","userId":"13680945168297461823"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n","\n","\n","print('DONE')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package google-drive-ocamlfuse.\n","(Reading database ... 131331 files and directories currently installed.)\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.6-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.6-0ubuntu1~ubuntu18.04.1) ...\n","Setting up google-drive-ocamlfuse (0.7.6-0ubuntu1~ubuntu18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n","DONE\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HcYsDS99WZKg","colab_type":"text"},"source":["**Make dir:**"]},{"cell_type":"code","metadata":{"id":"ew5fWm03PDfV","colab_type":"code","outputId":"9cd2b798-6745-49d3-8d3e-712b7bee3c61","executionInfo":{"status":"ok","timestamp":1563936916358,"user_tz":240,"elapsed":1918,"user":{"displayName":"Guoyao Shen","photoUrl":"https://lh5.googleusercontent.com/-n2iQosNzLWU/AAAAAAAAAAI/AAAAAAAAAAk/8oJH0jttUB8/s64/photo.jpg","userId":"13680945168297461823"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!mkdir -p drive\n","!google-drive-ocamlfuse -o nonempty drive\n","\n","print('DONE')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["DONE\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MdCw0gUsPGZV","colab_type":"text"},"source":["**Change Working Path:**"]},{"cell_type":"code","metadata":{"id":"YeYVlSypPKg-","colab_type":"code","outputId":"cde9cc0b-0906-448b-d8ad-f5262be13e19","executionInfo":{"status":"ok","timestamp":1563936919916,"user_tz":240,"elapsed":1472,"user":{"displayName":"Guoyao Shen","photoUrl":"https://lh5.googleusercontent.com/-n2iQosNzLWU/AAAAAAAAAAI/AAAAAAAAAAk/8oJH0jttUB8/s64/photo.jpg","userId":"13680945168297461823"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import os\n","\n","path = \"/content/drive/HourGlass_torch\"\n","# path = \"/content/gdrive/My Drive/HourGlass_torch\"\n","os.chdir(path)\n","os.listdir(path)\n","\n","print('DONE')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["DONE\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RoASmwU-PMwX","colab_type":"text"},"source":["Begin Net\n","===\n","**Import Packages:**"]},{"cell_type":"code","metadata":{"id":"iEKTwZf5PWpt","colab_type":"code","outputId":"30d43ab4-4d4b-4ba9-905f-8cccd57efeaa","executionInfo":{"status":"ok","timestamp":1563936934938,"user_tz":240,"elapsed":12183,"user":{"displayName":"Guoyao Shen","photoUrl":"https://lh5.googleusercontent.com/-n2iQosNzLWU/AAAAAAAAAAI/AAAAAAAAAAk/8oJH0jttUB8/s64/photo.jpg","userId":"13680945168297461823"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.data as datatorch\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import json\n","import os\n","import math\n","\n","from utils import imgutils\n","from models.hourglass import hg as hg_torch\n","from models.hourglass2 import hourglassnet as hgnet_torch\n","from losses.jointsmseloss import JointsMSELoss\n","from datasets.dataset_torch_new2 import DatasetTorch\n","\n","import os\n","\n","print('DONE')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["DONE\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hUQ_aJiNQp29","colab_type":"text"},"source":["**Conctruct Dataset:**"]},{"cell_type":"code","metadata":{"id":"oIO4sMmNQuub","colab_type":"code","colab":{}},"source":["# ================================== Construct dataset ==================================\n","num_bcsz = 16\n","\n","# ds_torch = DatasetTorch(use_flip=False, use_rand_color=True)  # Use True of rand_flip for this will cause error when inner default is False\n","ds_torch = DatasetTorch(use_flip=False, use_rand_color=True, use_randflipLR_inner=True)\n","data_loader = datatorch.DataLoader(ds_torch, batch_size=num_bcsz, shuffle=True)\n","\n","print('Construct dataset DONE')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IaEj0-ANQ2iK","colab_type":"text"},"source":["**Construct Model:**"]},{"cell_type":"code","metadata":{"id":"Q3huLw7EQ52_","colab_type":"code","cellView":"both","colab":{}},"source":["# ================================== Construct model ==================================\n","device = torch.device('cuda')\n","# device = torch.device('cpu')\n","learning_rate = 1e-3\n","\n","# net_hg_torch = hg_torch(num_stacks=4, num_blocks=1, num_classes=16).to(device)\n","net_hg_torch = hgnet_torch(num_stacks=2, num_blocks=1, num_classes=16, num_features=64).to(device)\n","optimizer = torch.optim.RMSprop(net_hg_torch.parameters(), lr=learning_rate)\n","# optimizer = torch.optim.Adam(net_hg_torch.parameters(), lr=learning_rate, weight_decay=0)\n","criteon = JointsMSELoss(use_target_weight=True).to(device)\n","\n","print('Construct model DONE')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OLdcQSWveCkA","colab_type":"text"},"source":["***Load Checkpoint and Recap Model, Optimizer, etc:***"]},{"cell_type":"code","metadata":{"id":"vJxlgrKuePgt","colab_type":"code","outputId":"32e3f196-bf87-471f-e7d1-9b4c7402a597","executionInfo":{"status":"ok","timestamp":1563936956873,"user_tz":240,"elapsed":11746,"user":{"displayName":"Guoyao Shen","photoUrl":"https://lh5.googleusercontent.com/-n2iQosNzLWU/AAAAAAAAAAI/AAAAAAAAAAk/8oJH0jttUB8/s64/photo.jpg","userId":"13680945168297461823"}},"colab":{"base_uri":"https://localhost:8080/","height":202}},"source":["# ================================== Construct dataset ==================================\n","num_bcsz = 16\n","\n","# ds_torch = DatasetTorch(use_flip=False, use_rand_color=True)  # Use True of rand_flip for this will cause error when inner default is False\n","ds_torch = DatasetTorch(use_flip=False, use_rand_color=True, use_randflipLR_inner=True)\n","data_loader = datatorch.DataLoader(ds_torch, batch_size=num_bcsz, shuffle=True)\n","\n","print('Construct dataset DONE')\n","\n","num_epoch = 20\n","num_setsize = ds_torch.__len__()\n","\n","# ================================== Reconstruct Net, Optim, Criteon ==================================\n","device = torch.device('cuda')\n","# device = torch.device('cpu')\n","learning_rate = 1e-3\n","\n","# net_hg_torch = hg_torch(num_stacks=8, num_blocks=1, num_classes=16)\n","# print('Reconstruct Model DONE')\n","# optimizer = torch.optim.RMSprop(net_hg_torch.parameters(), lr=learning_rate)\n","# print('Reconstruct Optimizer DONE')\n","criteon = JointsMSELoss(use_target_weight=True).to(device)\n","print('Reconstruct Criteon DONE')\n","\n","\n","# ================================== Reload Net, Optim ==================================\n","suffix = 'allEPOCH14'  # saved suffix to load\n","path_ckpt_torch = 'models/ckpt_hg_torch_' + str(suffix) + '.tar'\n","checkpoint = torch.load(path_ckpt_torch)\n","\n","# net_hg_torch = hg_torch(num_stacks=4, num_blocks=1, num_classes=16)\n","net_hg_torch = hgnet_torch(num_stacks=2, num_blocks=1, num_classes=16, num_features=64).to(device)\n","net_hg_torch = net_hg_torch.to(device)\n","net_hg_torch.load_state_dict(checkpoint['model_state_dict'])\n","print('Reconstruct Model DONE')\n","optimizer = torch.optim.RMSprop(net_hg_torch.parameters(), lr=learning_rate)\n","# optimizer = torch.optim.Adam(net_hg_torch.parameters(), lr=learning_rate, weight_decay=0)\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","print('Reconstruct Optimizer DONE')\n","LOSS = checkpoint['loss']\n","EPOCH = checkpoint['epoch']\n","STEP = checkpoint['step']\n","\n","net_hg_torch.train()\n","\n","print('===============CHECKPOINT LOADED===============')\n","print('CKPT LOSS', LOSS)\n","print('CKPT EPOCH', EPOCH)\n","print('CKPT STEP', STEP)\n","\n","# EPOCH = 0  # Reload EPOCH as file name\n","print('UPDATE EPOCH', EPOCH)\n","\n","# Calculate EPOCH and STEP left to go\n","# EPOCH_left = 20-(EPOCH)  # If not all epoch\n","EPOCH_left = 20-(EPOCH+1)  # If all epoch\n","STEP_left = math.ceil(num_setsize / num_bcsz)-(STEP+1)\n","print('LEFT CKPT EPOCH', EPOCH_left)\n","print('LEFT CKPT STEP', STEP_left)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Construct dataset DONE\n","Reconstruct Criteon DONE\n","Reconstruct Model DONE\n","Reconstruct Optimizer DONE\n","===============CHECKPOINT LOADED===============\n","CKPT LOSS tensor(0.0008, device='cuda:0', requires_grad=True)\n","CKPT EPOCH 14\n","CKPT STEP 1390\n","UPDATE EPOCH 14\n","LEFT CKPT EPOCH 5\n","LEFT CKPT STEP 0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kZDN3yGKUwhi","colab_type":"text"},"source":["**Train and Save Checkpoint:**"]},{"cell_type":"code","metadata":{"id":"dxGfxDZjU1Q_","colab_type":"code","outputId":"11fa48e1-7edb-4744-b6ed-ddc02e4900f6","executionInfo":{"status":"error","timestamp":1563791126873,"user_tz":240,"elapsed":14294576,"user":{"displayName":"Guoyao Shen","photoUrl":"https://lh5.googleusercontent.com/-n2iQosNzLWU/AAAAAAAAAAI/AAAAAAAAAAk/8oJH0jttUB8/s64/photo.jpg","userId":"13680945168297461823"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1rsNvu_BmVTbURbJuiicrXH10DmeQG2BU"}},"source":["# ================================== Train ==================================\n","# # First time train\n","# num_epoch = 20\n","# num_setsize = ds_torch.__len__()\n","# EPOCH = -1\n","\n","\n","# Reload CKPT and train\n","num_epoch = EPOCH_left\n","\n","\n","\n","print('num_setsize', num_setsize)\n","target_weight = np.array([[1.2, 1.1, 1, 1, 1.1, 1.2, 1, 1, 1, 1, 1.2, 1.1, 1, 1, 1.1, 1.2]])\n","target_weight = torch.from_numpy(target_weight).to(device).float()\n","\n","print('data_loader', data_loader)\n","\n","plt.ion()\n","for i in range(num_epoch):\n","    for step, (img, heatmaps, pts) in enumerate(data_loader):\n","        # To GPU\n","        img, heatmaps = img.to(device), heatmaps.cuda()\n","\n","        # All dtype change to float\n","        img, heatmaps = img.float(), heatmaps.float()\n","\n","        print('')\n","        print('EPOCH', str(num_epoch), '/', i + 1, ' ||  STEP', math.ceil(num_setsize / num_bcsz), '/', step + 1)\n","        \n","        heatmaps_pred_list = net_hg_torch(img)\n","        \n","#         # Intermediate Supervision\n","#         loss = torch.zeros([]).to(device)\n","#         for ele_heatmaps_pred in heatmaps_pred_list:\n","#             ele_loss = criteon(ele_heatmaps_pred, heatmaps, target_weight)\n","#             loss = loss + ele_loss\n","#         loss = loss / len(heatmaps_pred_list)\n","#         heatmaps_pred_final = heatmaps_pred_list[-1]\n","        \n","        # Only final loss, NO intermediate supervision\n","        heatmaps_pred_final = heatmaps_pred_list[-1]\n","        loss = criteon(heatmaps_pred_final, heatmaps, target_weight)\n","        \n","        \n","        print('LOSS:', loss)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        \n","        # Show last layer output\n","        heatmaps_pred_copy = heatmaps_pred_final[1]\n","        heatmaps_copy = heatmaps[1]\n","        img_copy = img[1]\n","\n","        # Show pred heatmaps\n","        heatmaps_pred_np = heatmaps_pred_copy.detach().cpu().numpy()\n","        heatmaps_pred_np = np.transpose(heatmaps_pred_np, (1, 2, 0))\n","        heatmaps_np = heatmaps_copy.detach().cpu().numpy()\n","        heatmaps_np = np.transpose(heatmaps_np, (1, 2, 0))\n","        img_np = img_copy.detach().cpu().numpy()\n","        img_np = np.transpose(img_np, (1, 2, 0))\n","        \n","        # In case too many figs to print\n","        if step % 5 == 0:\n","            imgutils.show_heatmaps(img_np, heatmaps_pred_np, num_fig=1)\n","            imgutils.show_heatmaps(img_np, heatmaps_np, num_fig=2)\n","#             plt.pause(0.1)\n","        print('===================================================')\n","        \n","        if (step % 300 == 0) and (step > 0):\n","            suffix_epoch = i+EPOCH+1\n","            suffix_step = step\n","            path_ckpt_torch = 'models/ckpt_hg_torch_EPOCH' + str(suffix_epoch) + 'STEP' + str(\n","                suffix_step) + '.tar'\n","            torch.save({\n","            'epoch': i+EPOCH+1,\n","            'step': step,\n","            'model_state_dict': net_hg_torch.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': loss\n","            }, path_ckpt_torch)\n","            print('===============CHECKPOINT PARAMS SAVED===============')\n","        del img, heatmaps, pts, heatmaps_pred_copy, heatmaps_copy,  img_copy, heatmaps_pred_np, heatmaps_np, img_np\n","\n","    # ================================== Save checkpoint each epoch ==================================\n","    suffix_epoch = i+EPOCH+1\n","    suffix_step = 0\n","    path_ckpt_torch = 'models/ckpt_hg_torch_allEPOCH' + str(suffix_epoch) + '.tar'\n","    torch.save({\n","            'epoch': i+EPOCH+1,\n","            'step': step,\n","            'model_state_dict': net_hg_torch.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': loss\n","            }, path_ckpt_torch)\n","    print('===============CHECKPOINT PARAMS SAVED===============')\n","plt.ioff()\n","\n","print('Training DONE')"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}